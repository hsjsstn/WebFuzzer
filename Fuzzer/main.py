import asyncio
import logging
import os
import sys
import urllib.robotparser
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

from crawler.static_crawler import StaticCrawler
from crawler.dynamic_crawler import crawl_dynamic
from fuzzing.async_fuzzer import AsyncFuzzer
from reporting.report_generator import generate_pdf_report

os.makedirs("results", exist_ok=True)
log_path = "results/fuzzer_logs.txt"

# ì»¤ìŠ¤í…€ í¬ë§·í„° í´ë˜ìŠ¤
class CustomFormatter(logging.Formatter):
    def __init__(self, fmt_with_ts, fmt_without_ts):
        super().__init__()
        self.fmt_with_ts = logging.Formatter(fmt_with_ts)
        self.fmt_without_ts = logging.Formatter(fmt_without_ts)

    def format(self, record):
        msg = record.getMessage().strip()  # ì•ë’¤ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°
        # íŠ¹ì • íƒœê·¸ê°€ í¬í•¨ëœ ë¡œê·¸ì—ë§Œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶œë ¥
        if any(tag in msg for tag in ['[StaticCrawler]', '[DynamicCrawler]', '[AsyncFuzzer]']):
            return self.fmt_with_ts.format(record)
        else:
            return self.fmt_without_ts.format(record)

# ë¡œê±° ì„¸íŒ…
logger = logging.getLogger("WebFuzzer")
logger.setLevel(logging.INFO)

if logger.hasHandlers():
    logger.handlers.clear()

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)

file_handler = logging.FileHandler(log_path, encoding='utf-8')
file_handler.setLevel(logging.INFO)

fmt_with_ts = '%(asctime)s - %(levelname)s - %(message)s'
fmt_without_ts = '%(message)s'

custom_formatter = CustomFormatter(fmt_with_ts, fmt_without_ts)

console_handler.setFormatter(custom_formatter)
file_handler.setFormatter(custom_formatter)

logger.addHandler(console_handler)
logger.addHandler(file_handler)


def print_banner():
    logger.info(
        "\n=================================================\n"
        "   ğŸ•·ï¸ WebFuzzer CLI - Intelligent Vulnerability Scanner\n"
        "================================================="
    )


def main(base_url=None, max_depth=None, selected_categories=None):
    print_banner()

    if base_url is None or max_depth is None or selected_categories is None:
        base_url = input("ğŸŒ í¬ë¡¤ë§ ì‹œì‘ URL (ì˜ˆ: http://localhost:4280): ").strip()
        if not base_url.startswith("http"):
            base_url = "http://" + base_url

        max_depth = int(input("ğŸ” ìµœëŒ€ í¬ë¡¤ë§ ê¹Šì´ (ì˜ˆ: 2): ").strip())

        all_categories = ['sql_injection', 'xss', 'command_injection', 'path_traversal', 'ssti', 'open_redirect', 'csrf']

        logger.info("ğŸ›¡ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ë¡œë“œ ìœ í˜•:")
        categories_text = "\n".join(f"- {c}" for c in all_categories)
        logger.info(categories_text)

        selected_input = input("\nğŸ¯ ì‚¬ìš©í•  í˜ì´ë¡œë“œ ìœ í˜• (ì½¤ë§ˆë¡œ êµ¬ë¶„): ").strip()
        selected_categories = [c.strip() for c in selected_input.split(',') if c.strip() in all_categories]

        if not selected_categories:
            logger.error("âŒ ìœ íš¨í•œ í˜ì´ë¡œë“œ ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit(1)

    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(urljoin(base_url, '/robots.txt'))
    try:
        rp.read()
    except:
        logger.warning("âš ï¸ robots.txt ë¡œë“œ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.")

    logger.info("ğŸ” ì •ì  í¬ë¡¤ë§ ì¤‘...")
    static_urls = StaticCrawler(base_url, rp).crawl()

    logger.info("ğŸ¥ ë™ì  í¬ë¡¤ë§ ì¤‘...")
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)

    visited, extraction = set(), []
    entry_url = list(static_urls)[0] if static_urls else base_url
    crawl_dynamic(driver, entry_url, max_depth, visited, extraction, rp)

    driver.quit()

    logger.info("ğŸ“ í¼ ìˆ˜ì§‘ ì¤‘...")
    forms = []
    for result in extraction:
        forms.extend(result['forms'])
        for field in result['independent_inputs']:
            if field.get('name'):
                forms.append({'action': result['url'], 'method': 'get', 'inputs': [field]})

    logger.info("ğŸš€ í¼ì§• ì‹œì‘...")
    if forms:
        fuzzer = AsyncFuzzer(forms, selected_categories)
        asyncio.run(fuzzer.run())
    else:
        logger.warning("âš ï¸ í¼ì§•í•  í¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        fuzzer = AsyncFuzzer(forms, selected_categories)
        fuzzer.vulnerabilities, fuzzer.attempts = [], []

    logger.info("ğŸ“„ PDF ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    generate_pdf_report(
        crawled_urls=static_urls.union(visited),
        extraction_results=extraction,
        vulnerabilities=fuzzer.vulnerabilities,
        attempts=fuzzer.attempts,
        output_path="results/fuzzer_report.pdf"
    )

    logger.info("âœ… í¼ì§• ì™„ë£Œ ë° ë¦¬í¬íŠ¸ ì €ì¥ë¨: results/fuzzer_report.pdf")

    return list(static_urls.union(visited)), extraction, fuzzer.vulnerabilities, fuzzer.attempts


if __name__ == "__main__":
    main()