import asyncio
import urllib.robotparser
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

from crawler.static_crawler import StaticCrawler
from crawler.dynamic_crawler import crawl_dynamic
from fuzzing.async_fuzzer import AsyncFuzzer
from reporting.report_generator import generate_pdf_report

def print_banner():
    print("""
=================================================
   ğŸ•·ï¸ WebFuzzer CLI - Intelligent Vulnerability Scanner
=================================================
    """)

# get_user_inputs í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•Šê³ , main()ì—ì„œ ì¸ìë¡œ ë°›ì•„ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½
# def get_user_inputs():
#     base_url = input("ğŸŒ í¬ë¡¤ë§ ì‹œì‘ URL (ì˜ˆ: http://localhost:4280): ").strip()
#     if not base_url.startswith("http"):
#         base_url = "http://" + base_url
#
#     max_depth = int(input("ğŸ” ìµœëŒ€ í¬ë¡¤ë§ ê¹Šì´ (ì˜ˆ: 2): ").strip())
#
#     all_categories = ['sql_injection', 'xss', 'command_injection', 'path_traversal', 'ssti', 'open_redirect', 'csrf']
#     print("\nğŸ›¡ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ë¡œë“œ ìœ í˜•:")
#     for c in all_categories:
#         print(f" - {c}")
#
#     selected_input = input("\nğŸ¯ ì‚¬ìš©í•  í˜ì´ë¡œë“œ ìœ í˜• (ì½¤ë§ˆë¡œ êµ¬ë¶„): ").strip()
#     selected_categories = [c.strip() for c in selected_input.split(',') if c.strip() in all_categories]
#
#     if not selected_categories:
#         print("âŒ ìœ íš¨í•œ í˜ì´ë¡œë“œ ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
#         exit(1)
#
#     return base_url, max_depth, selected_categories

# ğŸ”§ main() í•¨ìˆ˜ê°€ ì¸ìë¥¼ ì§ì ‘ ë°›ë„ë¡ ìˆ˜ì •
def main(base_url=None, max_depth=None, selected_categories=None):
    print_banner()

    # Flaskì—ì„œ ì§ì ‘ ë°›ì€ ì¸ìê°€ ì—†ë‹¤ë©´, ê¸°ì¡´ ì…ë ¥ ë°©ì‹ ìœ ì§€
    if base_url is None or max_depth is None or selected_categories is None:
        # base_url, max_depth, selected_categories = get_user_inputs()
        base_url = input("ğŸŒ í¬ë¡¤ë§ ì‹œì‘ URL (ì˜ˆ: http://localhost:4280): ").strip()
        if not base_url.startswith("http"):
            base_url = "http://" + base_url

        max_depth = int(input("ğŸ” ìµœëŒ€ í¬ë¡¤ë§ ê¹Šì´ (ì˜ˆ: 2): ").strip())

        all_categories = ['sql_injection', 'xss', 'command_injection', 'path_traversal', 'ssti', 'open_redirect', 'csrf']
        print("\nğŸ›¡ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ë¡œë“œ ìœ í˜•:")
        for c in all_categories:
            print(f" - {c}")

        selected_input = input("\nğŸ¯ ì‚¬ìš©í•  í˜ì´ë¡œë“œ ìœ í˜• (ì½¤ë§ˆë¡œ êµ¬ë¶„): ").strip()
        selected_categories = [c.strip() for c in selected_input.split(',') if c.strip() in all_categories]

        if not selected_categories:
            print("âŒ ìœ íš¨í•œ í˜ì´ë¡œë“œ ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit(1)

    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(urljoin(base_url, '/robots.txt'))
    try:
        rp.read()
    except:
        print("âš ï¸ robots.txt ë¡œë“œ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.")

    print("\nğŸ” ì •ì  í¬ë¡¤ë§ ì¤‘...")
    static_urls = StaticCrawler(base_url, rp).crawl()

    print("\nğŸ¥ ë™ì  í¬ë¡¤ë§ ì¤‘...")
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)

    visited, extraction = set(), []

    entry_url = list(static_urls)[0] if static_urls else base_url
    crawl_dynamic(driver, entry_url, max_depth, visited, extraction, rp)

    driver.quit()

    print("\nğŸ“ í¼ ìˆ˜ì§‘ ì¤‘...")
    forms = []
    for result in extraction:
        forms.extend(result['forms'])
        for field in result['independent_inputs']:
            if field.get('name'):
                forms.append({'action': result['url'], 'method': 'get', 'inputs': [field]})

    print("\nğŸš€ í¼ì§• ì‹œì‘...")
    if forms:
        fuzzer = AsyncFuzzer(forms, selected_categories)
        asyncio.run(fuzzer.run())
    else:
        print("âš ï¸ í¼ì§•í•  í¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        fuzzer = AsyncFuzzer(forms, selected_categories)
        fuzzer.vulnerabilities, fuzzer.attempts = [], []

    print("\nğŸ“„ PDF ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    import os
    os.makedirs("results", exist_ok=True)
    generate_pdf_report(
        crawled_urls=static_urls.union(visited),
        extraction_results=extraction,
        vulnerabilities=fuzzer.vulnerabilities,
        attempts=fuzzer.attempts,
        output_path="results/fuzzer_report.pdf"
    )

    print("\nâœ… í¼ì§• ì™„ë£Œ ë° ë¦¬í¬íŠ¸ ì €ì¥ë¨: results/fuzzer_report.pdf")

    return list(static_urls.union(visited)), extraction, fuzzer.vulnerabilities, fuzzer.attempts

if __name__ == "__main__":
    main()